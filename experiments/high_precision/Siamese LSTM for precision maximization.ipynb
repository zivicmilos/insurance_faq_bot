{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5bf09f91f91038d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    \"\"\"Preprocess and convert texts to a list of words\"\"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2890f4147926f697"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    \"\"\"Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "786bea22dfa0ca92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_answer(question: str) -> str:\n",
    "    df = pd.read_csv(\"../data/insurance_qna_dataset.csv\", sep=\"\\t\")\n",
    "    df.drop(columns=df.columns[0], axis=1, inplace=True)\n",
    "    answer = df.loc[df[\"Question\"] == question][\"Answer\"].tolist()\n",
    "\n",
    "    return \" \".join(answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86c05bec8bca94b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# File path\n",
    "TRAIN_TSV = \"../../data/quora_duplicate_questions.tsv\"\n",
    "WORD_VECTORS = \"custom\"  # word vectors strategy, e.g. \"custom\" or \"pretrained\"\n",
    "\n",
    "# Load training set\n",
    "train_df = pd.read_csv(TRAIN_TSV, delimiter=\"\\t\").iloc[:, 3:]\n",
    "\n",
    "df = pd.read_csv(\"../../data/insurance_qna_dataset.csv\", sep=\"\\t\")\n",
    "df.drop(columns=df.columns[0], axis=1, inplace=True)\n",
    "questions = np.unique(df.iloc[:, 0].to_numpy())\n",
    "documents = [\n",
    "    list(tokenize(question.lower())) for question in questions\n",
    "]\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = [\n",
    "    \"<unk>\"\n",
    "]  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "if WORD_VECTORS == \"custom\":\n",
    "    word2vec = Word2Vec(\n",
    "        sentences=documents, vector_size=100, window=5, min_count=1, workers=4, epochs=50\n",
    "    ).wv\n",
    "elif WORD_VECTORS == \"pretrained\":\n",
    "    word2vec = downloader.load(\"word2vec-google-news-300\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Word vectors {WORD_VECTORS} are not supported. Try 'custom' or 'pretrained'\"\n",
    "    )\n",
    "\n",
    "questions_cols = [\"question1\", \"question2\"]\n",
    "\n",
    "# Iterate over the questions only of training dataset\n",
    "for index, row in train_df.iterrows():\n",
    "    # Iterate through the text of both questions of the row\n",
    "    for question in questions_cols:\n",
    "        q2n = []  # q2n -> question numbers representation\n",
    "        for word in text_to_word_list(row[question]):\n",
    "            # Check for unwanted words\n",
    "            if word in stops and word not in word2vec.key_to_index:\n",
    "                continue\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "\n",
    "        # Replace questions as word to question as number representation\n",
    "        train_df.at[index, question] = q2n\n",
    "\n",
    "if WORD_VECTORS == \"custom\":\n",
    "    embedding_dim = 100\n",
    "elif WORD_VECTORS == \"pretrained\":\n",
    "    embedding_dim = 300\n",
    "\n",
    "embeddings = 1 * np.random.randn(\n",
    "    len(vocabulary) + 1, embedding_dim\n",
    ")  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embeddings[index] = word2vec.get_vector(word)\n",
    "\n",
    "del word2vec\n",
    "\n",
    "max_seq_length = max(\n",
    "    train_df.question1.map(lambda x: len(x)).max(),\n",
    "    train_df.question2.map(lambda x: len(x)).max(),\n",
    ")\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = 40000\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[questions_cols]\n",
    "Y = train_df[\"is_duplicate\"]\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
    "    X, Y, test_size=validation_size\n",
    ")\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {\"left\": X_train.question1, \"right\": X_train.question2}\n",
    "X_validation = {\"left\": X_validation.question1, \"right\": X_validation.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], [\"left\", \"right\"]):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train[\"left\"].shape == X_train[\"right\"].shape\n",
    "assert len(X_train[\"left\"]) == len(Y_train)\n",
    "\n",
    "# Model variables\n",
    "n_hidden = 20\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 1\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "right_input = Input(shape=(max_seq_length,), dtype=\"int32\")\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(embeddings),\n",
    "    embedding_dim,\n",
    "    weights=[embeddings],\n",
    "    input_length=max_seq_length,\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(\n",
    "    function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "    output_shape=lambda x: (x[0][0], 1),\n",
    ")([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "# # Adadelta optimizer, with gradient clipping by norm\n",
    "# optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "# \n",
    "# malstm.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# \n",
    "# # Start training\n",
    "# training_start_time = time()\n",
    "# \n",
    "# malstm_trained = malstm.fit(\n",
    "#     [X_train[\"left\"], X_train[\"right\"]],\n",
    "#     Y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=n_epoch,\n",
    "#     validation_data=([X_validation[\"left\"], X_validation[\"right\"]], Y_validation),\n",
    "# )\n",
    "# \n",
    "# print(\n",
    "#     \"Training time finished.\\n{} epochs in {}\".format(\n",
    "#         n_epoch, datetime.timedelta(seconds=time() - training_start_time)\n",
    "#     )\n",
    "# )\n",
    "# \n",
    "# if WORD_VECTORS == \"custom\":\n",
    "#     with open('../high_precision/vocabulary/vocabulary_custom_wv.pkl', 'wb') as f:\n",
    "#         pickle.dump(vocabulary, f)\n",
    "#     np.save(\"../high_precision/embeddings/custom_wv_embeddings.npy\", embeddings)\n",
    "#     malstm.save_weights(\"../high_precision/weights/malstm_weights_custom_wv.h5\")\n",
    "# elif WORD_VECTORS == \"pretrained\":\n",
    "#     with open('../high_precision/vocabulary/vocabulary_pretrained_wv.pkl', 'wb') as f:\n",
    "#         pickle.dump(vocabulary, f)\n",
    "#     np.save(\"../high_precision/embeddings/pretrained_wv_embeddings.npy\", embeddings)\n",
    "#     malstm.save_weights(\"../high_precision/weights/malstm_weights_pretrained_wv.h5\")\n",
    "# \n",
    "# # Plot accuracy\n",
    "# plt.plot(malstm_trained.history[\"accuracy\"])\n",
    "# plt.plot(malstm_trained.history[\"val_accuracy\"])\n",
    "# plt.title(\"Model Accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "# plt.show()\n",
    "# \n",
    "# # Plot loss\n",
    "# plt.plot(malstm_trained.history[\"loss\"])\n",
    "# plt.plot(malstm_trained.history[\"val_loss\"])\n",
    "# plt.title(\"Model Loss\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdac4362e3c76116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if question.model == \"custom\":\n",
    "    malstm.load_weights(\n",
    "        \"../experiments/high_precision/weights/malstm_weights_custom_wv.h5\"\n",
    "    )\n",
    "else:\n",
    "    malstm.load_weights(\n",
    "        \"../experiments/high_precision/weights/malstm_weights_pretrained_wv.h5\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bddb96a7dc9ee62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "high_recall_model = WordLevelVectorization(\n",
    "            train=False,\n",
    "            n_neighbours=100,\n",
    "            metric=\"cosine\",\n",
    "            logging=False,\n",
    "            word_vectors=\"custom\",\n",
    "            strategy=\"sum\",\n",
    "            weight=None,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6d722d5259ab3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candidates = high_recall_model.get_n_similar_documents(question.question)\n",
    "candidates_ = candidates.copy()\n",
    "questions = [question.question for _ in range(len(candidates))]\n",
    "\n",
    "for i, c in enumerate(candidates):\n",
    "    candidates[i] = [\n",
    "        vocabulary.get(word, 0)\n",
    "        for word in text_to_word_list(c)\n",
    "        if word not in stops\n",
    "    ]\n",
    "for i, q in enumerate(questions):\n",
    "    questions[i] = [\n",
    "        vocabulary.get(word, 0)\n",
    "        for word in text_to_word_list(q)\n",
    "        if word not in stops\n",
    "    ]\n",
    "candidates = pad_sequences(candidates, maxlen=212)\n",
    "questions = pad_sequences(questions, maxlen=212)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f324f3a6ee2baaa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = high_precision_model.predict([questions, candidates])\n",
    "output = list(itertools.chain.from_iterable(output))\n",
    "\n",
    "index_max = np.argmax(output)\n",
    "print(f\"Maximum of inference is: {output[index_max]}\")\n",
    "\n",
    "if output[index_max] < 0.7:\n",
    "    print(\"Sorry, but I do not understand your question. Can you rephrase it and try again?\")\n",
    "else:\n",
    "    print(find_answer(candidates_[index_max]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33daabc88173ab6b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
